{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "#     return filename\n",
    "def plot_eeg_1(db_for_vis_C_, generated_samples_BA_, db_for_vis_D_, generated_samples_AB_, subject_idx, epoch, save_dir='plots'):\n",
    "    num_trials = db_for_vis_C_.shape[0]\n",
    "    fig, axs = plt.subplots(4, 2, figsize=(30, 22))\n",
    "    \n",
    "    # Sử dụng f-string để thay thế subject_idx trong đường dẫn\n",
    "    save_dir = f\"C:/Users/VU NGOC TAM/Downloads/gan1/1 times/26092024-kernel 2,4-sub {subject_idx}\"\n",
    "\n",
    "    plt.style.use('seaborn-white')\n",
    "    for i in range(4):\n",
    "        axs[i][0].plot(db_for_vis_C_[i, :], label='truth')\n",
    "        axs[i][0].plot(generated_samples_BA_[i, :], label='generated')\n",
    "        axs[i][0].set_title('EEG and NIRS->EEG (channel {} )'.format(i + 1), size=23)\n",
    "\n",
    "        axs[i][1].plot(db_for_vis_D_[i, :], label='truth')\n",
    "        axs[i][1].plot(generated_samples_AB_[i, :], label='generated')\n",
    "        axs[i][1].set_title('NIRS and EEG->NIRS (channel {} )'.format(i + 1), size=23)\n",
    "\n",
    "    axs[3][0].set_xlabel('Time samples', fontsize=23.0)\n",
    "    axs[3][1].set_xlabel('Time samples', fontsize=23.0)\n",
    "\n",
    "    legend_labels = ['ground truth', 'generated']\n",
    "    for ax_row in axs:\n",
    "        for ax in ax_row:\n",
    "            ax.tick_params(axis='x', labelsize=23)\n",
    "            ax.tick_params(axis='y', labelsize=23)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.legend(legend_labels, loc='center', fontsize=16, borderaxespad=0.01, bbox_to_anchor=(0.5125, 0.49), shadow=True, bbox_transform=plt.gcf().transFigure)\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Sử dụng f-string để lưu file với giá trị subject_idx và epoch\n",
    "    filename = f\"{save_dir}/plot_epoch_{epoch}.png\"\n",
    "    plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return filename\n",
    "\n",
    "\n",
    "################################################################################################################################################ bắt đầu vô train\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Function to save lists to txt files with directory specified\n",
    "def save_to_txt(data, directory, filename):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    file_path = os.path.join(directory, filename)\n",
    "    with open(file_path, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "# Compute test loss function\n",
    "def compute_test_loss(model, xA, xB, iterr):\n",
    "    loss_G_test = model.train_gen_test(xA, xB, iterr)\n",
    "    loss_D_test = model.train_discrim(xA, xB, iterr)\n",
    "    return loss_G_test, loss_D_test\n",
    "\n",
    "# Main training loop\n",
    "global_epoch = 400\n",
    "total_epoch = 251\n",
    "batch_size = 30\n",
    "ckpt_dir = os.getcwd() + '/checkpoint/'\n",
    "vis_num = 4\n",
    "\n",
    "# Chia dữ liệu thành các nhóm 30 trials\n",
    "num_trials_per_subject = 30\n",
    "num_folds = eeg_normalize.shape[0] // num_trials_per_subject\n",
    "\n",
    "# Tạo thư mục để lưu kết quả nếu chưa tồn tại\n",
    "results_dir = \"C:/Users/VU NGOC TAM/Downloads/gan1/result cross\"\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "num_subjects = 29\n",
    "num_trials_per_subject = 30\n",
    "\n",
    "# Vòng lặp để chia dữ liệu thành các phần như mô tả\n",
    "for subject_idx in range(num_subjects):\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    start_idx = subject_idx * num_trials_per_subject\n",
    "    end_idx = start_idx + num_trials_per_subject\n",
    "    \n",
    "    # Chia tập kiểm thử và tập huấn luyện\n",
    "    test_indices = np.arange(start_idx, end_idx)\n",
    "    train_indices = np.setdiff1d(np.arange(eeg_normalize.shape[0]), test_indices)\n",
    "    \n",
    "    print(f\"Đang test với subject thứ {subject_idx + 1}, trial {start_idx} đến {end_idx - 1}\")\n",
    "    print(f\"Đang train với subject thứ {subject_idx + 1}, trial indices: {train_indices}\")\n",
    "    train_eeg = eeg_normalize[train_indices]\n",
    "    train_nirs = nirs_normalized[train_indices]\n",
    "\n",
    "    test_eeg = eeg_normalize[test_indices]\n",
    "    test_nirs = nirs_normalized[test_indices]\n",
    "\n",
    "    database_A = db.DBreader(train_eeg, batch_size=batch_size)\n",
    "    database_B = db.DBreader(train_nirs, batch_size=batch_size)\n",
    "    db_for_vis_A = train_eeg\n",
    "    db_for_vis_B = train_nirs\n",
    "\n",
    "    database_C = db.DBreader(test_eeg, batch_size=batch_size)\n",
    "    database_D = db.DBreader(test_nirs, batch_size=batch_size)\n",
    "    db_for_vis_C = test_eeg\n",
    "    db_for_vis_D = test_nirs\n",
    "\n",
    "\n",
    "\n",
    "    global_epoch = tf.Variable(0, trainable=False, name='global_step')\n",
    "    global_epoch_increase = tf.compat.v1.assign(global_epoch, tf.add(global_epoch, 1))\n",
    "    config = tf.compat.v1.ConfigProto(device_count={'GPU': 1, 'CPU': 12})\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.compat.v1.Session(config=config)\n",
    "    model = Discogan(sess, batch_size, size_=270)\n",
    "\n",
    "    saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables())\n",
    "\n",
    "\n",
    "    \n",
    "    total_batch_A = database_A.total_batch\n",
    "    total_batch_B = database_B.total_batch\n",
    "\n",
    "    if total_batch_A > total_batch_B:\n",
    "        total_batch = total_batch_B\n",
    "    else:\n",
    "        total_batch = total_batch_A\n",
    "\n",
    "    discriminator_loss = []\n",
    "    generator_loss = []\n",
    "    corr_A_ABA = []\n",
    "    corr_B_BAB = []\n",
    "    total_corr_A_ABA = []\n",
    "    total_corr_B_BAB = []\n",
    "    corr_A_ABA_train_1_2 = []\n",
    "    corr_B_BAB_train_1_2 = []\n",
    "    total_corr_A_ABA_train_1_2 = []\n",
    "    total_corr_B_BAB_train_1_2 = []\n",
    "    test_losses_G_per_epoch = []\n",
    "    test_losses_D_per_epoch = []\n",
    "    \n",
    "    corr_A_ABA_1= []\n",
    "    total_corr_A_ABA_1= []\n",
    "    corr_B_BAB_1= []\n",
    "    total_corr_B_BAB_1= []\n",
    "    corr_A_ABA_train_1= []\n",
    "    total_corr_A_ABA_train_1= []\n",
    "    corr_B_BAB_train_1= []\n",
    "    total_corr_B_BAB_train_1= []\n",
    "\n",
    "\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    epoch = sess.run(global_epoch)\n",
    "    print('HERE>>>>>>>>>>>>>>>>>>>')\n",
    "    print(epoch)\n",
    "    output_directory = f\"C:/Users/VU NGOC TAM/Downloads/gan1/1 times/26092024-kernel 2,4-sub {subject_idx + 1}/output_txt\"\n",
    "\n",
    "    while True:\n",
    "        if epoch == total_epoch:\n",
    "            break\n",
    "        for step in range(total_batch):\n",
    "            input_A = database_A.next_batch()\n",
    "            input_B = database_B.next_batch()\n",
    "\n",
    "            input_C = database_C.next_batch()\n",
    "            input_D = database_D.next_batch()\n",
    "\n",
    "            if step % 2 == 0:\n",
    "                loss_D = model.train_discrim_test(input_C, input_D, epoch * total_batch + step)  # Train Discriminator and get the loss value\n",
    "                loss_G_test, loss_D_test = compute_test_loss(model, input_C, input_D, epoch * total_batch + step)\n",
    "            loss_G = model.train_gen_test(input_C, input_D, epoch * total_batch + step)  # Train Generator and get the loss value\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print('Epoch: [', epoch, '/', total_epoch, '], ', 'Step: [', step, '/', total_batch, '], D_loss: ', loss_D, ', G_loss: ', loss_G)\n",
    "                discriminator_loss.append(loss_D)\n",
    "                generator_loss.append(loss_G)\n",
    "                test_losses_G_per_epoch.append(loss_G_test)\n",
    "                test_losses_D_per_epoch.append(loss_D_test)\n",
    "\n",
    "            if step % 500 == 0:\n",
    "                generated_samples_AB = model.sample_generate(db_for_vis_C, 'AB',batch_size=30)\n",
    "                generated_samples_ABA = model.sample_generate(db_for_vis_C, 'ABA',batch_size=30)\n",
    "                generated_samples_BA = model.sample_generate(db_for_vis_D, 'BA',batch_size=30)\n",
    "                generated_samples_BAB = model.sample_generate(db_for_vis_D, 'BAB',batch_size=30)\n",
    "\n",
    "                generated_samples_AB_ = np.mean(np.array(generated_samples_AB), axis=0)\n",
    "                generated_samples_ABA_ = np.mean(np.array(generated_samples_ABA), axis=0)\n",
    "                generated_samples_BA_ = np.mean(np.array(generated_samples_BA), axis=0)\n",
    "                generated_samples_BAB_ = np.mean(np.array(generated_samples_BAB), axis=0)\n",
    "\n",
    "                db_for_vis_C_ = np.mean(np.array(db_for_vis_C), axis=0)\n",
    "                db_for_vis_D_ = np.mean(np.array(db_for_vis_D), axis=0)\n",
    "\n",
    "                img_for_vis_AB = np.concatenate([db_for_vis_C, generated_samples_AB, generated_samples_ABA], axis=2)\n",
    "                img_for_vis_BA = np.concatenate([db_for_vis_D, generated_samples_BA, generated_samples_BAB], axis=2)\n",
    "\n",
    "                corr_a_test = correlation(db_for_vis_C_, generated_samples_ABA_)\n",
    "                corr_A_ABA.append(corr_a_test)\n",
    "                total_corr_A_ABA.append(np.mean(corr_a_test))\n",
    "                corr_b_test = correlation(db_for_vis_D_, generated_samples_BAB_)\n",
    "                corr_B_BAB.append(corr_b_test)\n",
    "                total_corr_B_BAB.append(np.mean(corr_b_test))\n",
    "\n",
    "                corr_a_train_1_2 = correlation(db_for_vis_C_, generated_samples_BA_)\n",
    "                corr_A_ABA_train_1_2.append(corr_a_train_1_2)\n",
    "                total_corr_A_ABA_train_1_2.append(np.mean(corr_a_train_1_2))\n",
    "                corr_b_train_1_2 = correlation(db_for_vis_D_, generated_samples_AB_)\n",
    "                corr_B_BAB_train_1_2.append(corr_b_train_1_2)\n",
    "                total_corr_B_BAB_train_1_2.append(np.mean(corr_b_train_1_2))\n",
    "\n",
    "\n",
    "        epoch = sess.run(global_epoch_increase)\n",
    "        save_to_txt(test_losses_G_per_epoch, output_directory, \"test_losses_G_per_epoch.txt\")\n",
    "        save_to_txt(test_losses_D_per_epoch, output_directory, \"test_losses_D_per_epoch.txt\")\n",
    "        save_to_txt(total_corr_A_ABA_train_1_2, output_directory, \"total_corr_A_ABA_train_1_Correlation.txt\")\n",
    "        save_to_txt(total_corr_B_BAB_train_1_2, output_directory, \"total_corr_B_BAB_train_1_Correlation.txt\")\n",
    "\n",
    "\n",
    "        trial_index = 9\n",
    "        # plot_eeg_2(db_for_vis_C_, generated_samples_ABA_, db_for_vis_D_, generated_samples_BAB_)\n",
    "        if epoch % 50 == 0:\n",
    "            plot_eeg_1(db_for_vis_C_, generated_samples_BA_, db_for_vis_D_, generated_samples_AB_, subject_idx + 1, epoch)\n",
    "########################################################################################################################################  vẽ hình thôi nha a\n",
    "        # plot_3_fig(db_for_vis_C_, generated_samples_AB_, generated_samples_ABA_, db_for_vis_D_, generated_samples_BA_, generated_samples_BAB_, save_dir='plots')\n",
    "        base_dir = f\"C:/Users/VU NGOC TAM/Downloads/gan1/1 times/26092024-kernel 2,4-sub {subject_idx + 1}\"\n",
    "        subject_dir = os.path.join(base_dir, f\"{subject_idx + 1}\")\n",
    "\n",
    "        # Tạo thư mục nếu chưa tồn tại\n",
    "        if not os.path.exists(subject_dir):\n",
    "            os.makedirs(subject_dir)\n",
    "        plotting_data = [test_losses_D_per_epoch,test_losses_G_per_epoch,total_corr_A_ABA_train_1_2, total_corr_B_BAB_train_1_2]\n",
    "\n",
    "        graph_titles = ['Discriminator loss test 1 times',\n",
    "                             'Generator loss test 1 times',\n",
    "                        'Mean of Correlation between EEG and NIRS->EEG',\n",
    "                        'Mean of Correlation between NIRS and EEG->NIRS']\n",
    "        y_axis_titles = list(['Loss'] * 2 +\n",
    "                             ['Correlation Coefficient'] * 4)\n",
    "        fig, axs = plt.subplots(int(len(plotting_data) / 2), 2, figsize=(24, 12), sharey='row', facecolor='w', edgecolor='k')\n",
    "        fig.subplots_adjust(hspace=.30, wspace=.20)\n",
    "        axs = axs.ravel()\n",
    "        for i, (title, data) in enumerate(zip(graph_titles, plotting_data)):\n",
    "            if np.shape(np.shape(data))[0] == 1:\n",
    "                axs[i].plot(data)\n",
    "            else:\n",
    "                for result in data:\n",
    "                    axs[i].plot(result)\n",
    "            axs[i].set_title(title, fontweight=\"bold\", size=22)\n",
    "            axs[i].set_xlabel('Number of Epochs', fontsize=20.0)\n",
    "            axs[i].set_ylabel(y_axis_titles[i], fontsize=20.0)\n",
    "            axs[i].tick_params(axis='y', which='both', left=True, right=False, labelleft=True, labelsize=18)\n",
    "            axs[i].tick_params(axis='x', which='both', bottom=True, top=False, labelbottom=True, labelsize=18)\n",
    "        # fig.savefig(f'loss_corrcoef_subject_{subject_idx + 1}_{total_epoch}epochs.png', pad_inches=0.015, bbox_inches='tight')\n",
    "        loss_corrcoef_path = os.path.join(subject_dir, f\"loss_corrcoef_subject_{subject_idx + 1}_{total_epoch}epochs.png\")\n",
    "        fig.savefig(loss_corrcoef_path, pad_inches=0.015, bbox_inches='tight')\n",
    "\n",
    "        plotting_data_test = [test_losses_D_per_epoch, test_losses_G_per_epoch, total_corr_A_ABA, total_corr_B_BAB]\n",
    "        graph_titles_test = ['Discriminator loss test 2 times',\n",
    "                             'Generator loss test 2 times', 'Mean of Correlation between EEG and EEG->NIRS->EEG',\n",
    "                             'Mean of Correlation between NIRS and NIRS->EEG->NIRS']\n",
    "        y_axis_titles = list(['Loss'] * 2 +\n",
    "                             ['Correlation Coefficient'] * 4)\n",
    "        fig, axs = plt.subplots(int(len(plotting_data_test) / 2), 2, figsize=(24, 12), sharey='row', facecolor='w', edgecolor='k')\n",
    "        fig.subplots_adjust(hspace=.30, wspace=.20)\n",
    "        axs = axs.ravel()\n",
    "        for i, (title, data) in enumerate(zip(graph_titles_test, plotting_data_test)):\n",
    "            if np.shape(np.shape(data))[0] == 1:\n",
    "                axs[i].plot(data)\n",
    "            else:\n",
    "                for result in data:\n",
    "                    axs[i].plot(result)\n",
    "            axs[i].set_title(title, fontweight=\"bold\", size=22)\n",
    "            axs[i].set_xlabel('Number of Epochs', fontsize=20.0)\n",
    "            axs[i].set_ylabel(y_axis_titles[i], fontsize=20.0)\n",
    "            axs[i].tick_params(axis='y', which='both', left=True, right=False, labelleft=True, labelsize=18)\n",
    "            axs[i].tick_params(axis='x', which='both', bottom=True, top=False, labelbottom=True, labelsize=18)\n",
    "        # fig.savefig(f'test_loss_subject_{subject_idx + 1}_{total_epoch}epochs.png', pad_inches=0.015, bbox_inches='tight')\n",
    "        \n",
    "        \n",
    "        test_loss_path = os.path.join(subject_dir, f\"test_loss_subject_{subject_idx + 1}_{total_epoch}epochs.png\")\n",
    "        fig.savefig(test_loss_path, pad_inches=0.015, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "################################################################################################################################################\n",
    "    sess.close()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
